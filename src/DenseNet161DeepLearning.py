# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R9aWvJ7D1mjEPUB8aMnv6FShfGVjQsKk
"""

import torch
import torch.nn as nn
import torchvision
from torch import optim
import torch.nn.functional as F
from PIL import Image

import copy


import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torchvision import datasets


from torchvision.models import densenet161


class DenseNet161:
    def __init__(self):
        self.device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")
        self.data_dir = 'chest_xray'
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
        ])
        # Create model with pretrained weights
        self.model = densenet161(pretrained=True)
        self.model.classifier = nn.Linear(2208, 2)  # DenseNet161 has 2208 features
        self.model = self.model.to(self.device)

        self.best_val_loss = float('inf')
        
        self.activation = {}

    def print_model(self):
        print(self.model)

    def check_frozen_layer(self):
        model = self.model
        print("CHECKING FROZEN AND UNFROZEN LAYERS")
        print("================================================================================")
        print()
        print()
        # Check for frozen layers
        for name, param in model.named_parameters():
            if param.requires_grad == False:
                print(f" {name} is frozen")
            else:
                print(f" {name} is unfrozen")

        print("================================================================================")

    def init_dataloader(self):

        self.train_dataset = datasets.ImageFolder(f'{self.data_dir}/train', transform=self.transform)
        self.val_dataset   = datasets.ImageFolder(f'{self.data_dir}/val', transform=self.transform)
        self.test_dataset  = datasets.ImageFolder(f'{self.data_dir}/test', transform=self.transform)

        self.train_loader = DataLoader(self.train_dataset, batch_size=32, shuffle=True)
        self.val_loader   = DataLoader(self.val_dataset, batch_size=32)
        self.test_loader  = DataLoader(self.test_dataset, batch_size=1, shuffle=True)

    def print_dataset_class_info(self):
        # Print dataset sizes
        print("\nDataset sizes:")
        print(f"Training set: {len(self.train_dataset)} images")
        print(f"Validation set: {len(self.val_dataset)} images")
        print(f"Test set: {len(self.test_dataset)} images")

        # Access class information
        print(f"Classes: {self.train_dataset.classes}")  # ['NORMAL', 'PNEUMONIA']
        print(f"Class mapping: {self.train_dataset.class_to_idx}")  # {'NORMAL': 0, 'PNEUMONIA': 1}

    def test_model(self,model_input, test_loader_input):

            model = model_input
            test_loader = test_loader_input

            test_loss = 0.0
            correct = 0
            total = 0

            # Set model to evaluation mode
            model.eval()

            with torch.no_grad():
                for inputs, labels in test_loader:
                    inputs, labels = inputs.to(self.device), labels.to(self.device)

                    # Forward pass
                    outputs = model(inputs)
                    loss = F.cross_entropy(outputs, labels)
                    test_loss += loss.item()

                    # (value, indices)
                    _, preds = torch.max(outputs, 1)
                    correct += (preds == labels).sum().item()
                    total += labels.size(0)

            # Calculate final metrics
            test_loss = test_loss / len(test_loader)
            test_accuracy = correct / total


            return test_loss, test_accuracy


    def eval_model(self, model_input, eval_loader_input):

        model = model_input
        eval_loader = eval_loader_input

        # Initialize variables for tracking performance

        model.eval()

        running_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in eval_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = model(inputs)
                loss = F.cross_entropy(outputs, labels)
                running_loss += loss.item()

                # Get predictions
                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        # Calculate metrics
        eval_loss = running_loss / len(eval_loader)
        eval_accuracy = correct / total



        return eval_loss, eval_accuracy

    def train_model(self,model_input, train_loader_input, val_loader_input, optimizer_input, epochs=20):

        model = model_input
        train_loader = train_loader_input
        val_loader = val_loader_input
        optimizer = optimizer_input

        # Training history
        train_losses = []
        train_accuracies = []
        val_losses = []
        val_accuracies = []

        for epoch in range(epochs):
            # Training phase
            model.train()
            running_loss = 0.0
            train_correct = 0
            train_total = 0

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                print(inputs.shape)
                print(labels.shape)
                # Zero gradients
                optimizer.zero_grad()

                # Forward pass
                outputs = model(inputs)
                loss = F.cross_entropy(outputs, labels)

                # Backward pass
                loss.backward()
                optimizer.step()

                # Track metrics
                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()

            # Calculate epoch metrics
            epoch_loss = running_loss / len(train_loader)
            epoch_acc = train_correct / train_total
            train_losses.append(epoch_loss)
            train_accuracies.append(epoch_acc)

            # Validation phase
            val_loss, val_acc = self.eval_model(model, val_loader)
            val_losses.append(val_loss)
            val_accuracies.append(val_acc)

            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                print("======================================================================================================")
                print("saving best model......")
                print("======================================================================================================")

                # Save relevant information
                checkpoint = {
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'epoch': epoch,
                    'val_loss': val_loss,
                    'val_acc': val_acc,
                    'train_losses': train_losses,
                    'val_losses': val_losses
                }

                torch.save(checkpoint, 'best_model_full.pt')



            # Print epoch results
            print(f'Epoch {epoch + 1}/{epochs}:')
            print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\n')

        return {
            'train_losses': train_losses,
            'train_accuracies': train_accuracies,
            'val_losses': val_losses,
            'val_accuracies': val_accuracies
        }

    def print_model_parameters(self):
        
        print("="*50)
        print(f'PARAMETERS BY LAYER')
        print("="*50)
        
        trainable_params = 0
        non_trainable_params = 0
    
        # Iterate through all parameters
        for param in self.model.parameters():
            if param.requires_grad:
                trainable_params += param.numel()
            else:
                non_trainable_params += param.numel()
                
        return trainable_params, non_trainable_params
    

    def progressive_training_DenseNet161(self,model_input, train_loader_input, val_loader_input):

        model = model_input
        train_loader = train_loader_input
        val_loader = val_loader_input

        for param in model.parameters():
            param.requires_grad = False

        print("Unfreezing denseblock1...")
        for param in model.features.denseblock1.parameters():
            param.requires_grad = True

        print("Unfreezing transition1...")
        print()
        print()
        for param in model.features.transition1.parameters():
            param.requires_grad = True

        optimizer = optim.SGD([
            {'params': model.features.denseblock1.parameters(), 'lr': 0.02},
            {'params': model.features.transition1.parameters(), 'lr': 0.05}
        ])

        # Train
        print()
        print()
        print("=======================================================================")
        print("Training denseblock1 and transition1...")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=2)

        #Freeze all
        for param in model.parameters():
            param.requires_grad = False

        print("Unfreezing denseblock2...")
        for param in model.features.denseblock2.parameters():
            param.requires_grad = True

        # optimizer
        optimizer = optim.SGD(
            model.features.denseblock2.parameters(), lr = 0.02)

        # Train
        print("=======================================================================")
        print("Training denseblock2...")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=3)

        #Freeze all
        for param in model.parameters():
            param.requires_grad = False

        print("Unfreezing transition2...")
        for param in model.features.transition2.parameters():
            param.requires_grad = True

        print("Unfreezing denseblock3...")
        for param in model.features.denseblock3.parameters():
            param.requires_grad = True

        optimizer = optim.SGD([
            {'params': model.features.transition2.parameters(), 'lr': 0.05},
            {'params': model.features.denseblock3.parameters(), 'lr': 0.02}
        ])

        # Train
        print("=======================================================================")
        print("Training transition2 and denseblock3")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=3)

        #Freeze all
        for param in model.parameters():
            param.requires_grad = False

        print("Unfreezing transition3...")
        for param in model.features.transition3.parameters():
            param.requires_grad = True

        print("Unfreezing denseblock4...")
        for param in model.features.denseblock4.parameters():
            param.requires_grad = True

        # Initial optimizer
        optimizer = optim.SGD([
            {'params': model.features.transition3.parameters(), 'lr': 0.05},
            {'params': model.features.denseblock4.parameters(), 'lr': 0.02}
        ])

        #Train
        print("=======================================================================")
        print("Training transition3 and denseblock4")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=3)

        # Freeze all
        for param in model.parameters():
            param.requires_grad = False

        # Unfreeze norm5
        for param in model.features.norm5.parameters():
            param.requires_grad = True

        # Unfreeze classifier
        for param in model.classifier.parameters():
            param.requires_grad = True

        # Initial optimizer
        optimizer = optim.SGD([
            {'params': model.features.norm5.parameters(), 'lr': 0.05},
            {'params': model.classifier.parameters(), 'lr': 0.02}
        ])


        #Train
        print("Training norm5 and classifier")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=4)
        print("=======================================================================")


    def load_and_use_model_DenseNet(self,path = 'best_model_full_DenseNet.pt', device=torch.device("cpu")):

        #Set up device
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        #Load the checkpoint
        checkpoint = torch.load(path)

        #Initialize the model architecture
        model = densenet161(pretrained=True)
        model.classifier = nn.Linear(2208, 2)
        model = model.to(device)

        #Load the saved state dict
        model.load_state_dict(checkpoint['model_state_dict'])

        #Set model to evaluation mode
        model.eval()

        #Transform for new images
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
        ])

        return model, transform
    
    def print_checkpoint_values(self, checkpoint_path='best_model_full_DenseNet.pt'):

        # Load checkpoint
        checkpoint = torch.load(checkpoint_path)
        
        print("="*50)
        print("CHECKPOINT VALUES")
        print("="*50)
        
        # Print training info
        print(f"Best epoch: {checkpoint['epoch']}")
        print(f"Validation Loss: {checkpoint['val_loss']:.4f}")
        print(f"Validation Accuracy: {checkpoint['val_acc']:.4f}")

    def getActivation(self, name):
        def hook(module, input, output):
            self.activation[name] = output.detach()
        return hook


    def predict_image(self, model, test_loader_input):
        test_loader = test_loader_input
        model.features.denseblock4.denselayer24.conv2.register_forward_hook(self.getActivation('final_conv'))
        # Load and preprocess the image
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        for image, label in test_loader:
            image_tensor = image.to(device)

            # Make prediction
            with torch.no_grad():
                outputs = model(image_tensor)
                _, predicted = torch.max(outputs, 1)
                
                
                # Fetch the learned weights at the final feed-forward layer
                weight_fc = model.classifier.weight.detach().numpy()
                print(weight_fc.shape)


                # Fetch feature maps at the final convolutional layer
                conv_features = self.activation['final_conv']
                print(f"Feature map shape: {conv_features.shape}")
                print(f"Model predicted value {predicted.item()}")
                print(f"Actual value {label.item()}")
                print("-" * 50)
                
    def collect_model_info(self, model_input, test_loader_input):
        
        model = model_input
        test_loader = test_loader_input

        # 1. Model Architecture
        print("Model Architecture:")
        print(model)
        print("================================================================================")

        # 2. Check Frozen Layers
        print("CHECKING FROZEN AND UNFROZEN LAYERS")
        print("================================================================================")
        for name, param in model.named_parameters():
            if not param.requires_grad:
                print(f"Layer {name} is frozen")
            else:
                print(f"Layer {name} is unfrozen")
        print("================================================================================")

        # 3. Dataset Class Information
        print("\nDataset sizes:")
        print(f"Training set: {len(self.train_dataset)} images")
        print(f"Validation set: {len(self.val_dataset)} images")
        print(f"Test set: {len(self.test_dataset)} images")
        print(f"Classes: {self.train_dataset.classes}")
        print(f"Class mapping: {self.train_dataset.class_to_idx}")
        print("================================================================================")

        # 4. Test Loss and Accuracy
        test_loss, test_accuracy = self.test_model(model, test_loader)
        print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")
        print("================================================================================")



def main():

    model = DenseNet161()
    model.init_dataloader()
    test_loader = model.test_loader
    model_fined_tuned, transform = model.load_and_use_model_DenseNet()
    model.collect_model_info(model_fined_tuned, test_loader)


# def main():
#     model = DenseNet161()
    
#     model.print_checkpoint_values()

# def main():

#     model = DenseNet161()
#     model.init_dataloader()
#     model_fined_tuned, transform = model.load_and_use_model_DenseNet()
#     print(model_fined_tuned)
    
    
# def main():

#     model = DenseNet161()
#     model.init_dataloader()
#     test_loader = model.test_loader
#     model_fined_tuned, transform = model.load_and_use_model_DenseNet()
#     model.predict_image(model_fined_tuned, test_loader)

# def main():

#     model = DenseNet161()
#     model.init_dataloader()
#     test_loader = model.test_loader
#     model.print_model()
#     model.check_frozen_layer()
#     model.print_dataset_class_info()
#     model.progressive_training_DenseNet161(model.model, model.train_loader, model.val_loader)
#     model.test_model(model.model, test_loader)
    
    
if __name__ == '__main__':
    main()
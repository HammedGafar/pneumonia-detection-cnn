# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R9aWvJ7D1mjEPUB8aMnv6FShfGVjQsKk
"""

import torch
import torch.nn as nn
import torchvision
from torch import optim
import torch.nn.functional as F
from PIL import Image

import copy

import sklearn
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay

import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torchvision import datasets

import matplotlib.pyplot as plt
import numpy as np

from torchvision.models import densenet161


class DenseNet161:
    def __init__(self):
        self.device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")
        self.data_dir = 'chest_xray'
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
        ])
        # Create model with pretrained weights
        self.model = densenet161(pretrained=True)
        self.model.classifier = nn.Linear(2208, 2)  # DenseNet161 has 2208 features
        self.model = self.model.to(self.device)

        self.best_val_loss = float('inf')
        
        self.activation = {}

    def print_model(self):
        print(self.model)

    def check_frozen_layer(self):
        model = self.model
        print("CHECKING FROZEN AND UNFROZEN LAYERS")
        print("================================================================================")
        print()
        print()
        # Check for frozen layers
        for name, param in model.named_parameters():
            if param.requires_grad == False:
                print(f" {name} is frozen")
            else:
                print(f" {name} is unfrozen")

        print("================================================================================")

    def init_dataloader(self):

        self.train_dataset = datasets.ImageFolder(f'{self.data_dir}/train', transform=self.transform)
        self.val_dataset   = datasets.ImageFolder(f'{self.data_dir}/val', transform=self.transform)
        self.test_dataset  = datasets.ImageFolder(f'{self.data_dir}/test', transform=self.transform)

        self.train_loader = DataLoader(self.train_dataset, batch_size=32, shuffle=True)
        self.val_loader   = DataLoader(self.val_dataset, batch_size=32)
        self.test_loader  = DataLoader(self.test_dataset, batch_size=1, shuffle=True)

    def print_dataset_class_info(self):
        # Print dataset sizes
        print("\nDataset sizes:")
        print(f"Training set: {len(self.train_dataset)} images")
        print(f"Validation set: {len(self.val_dataset)} images")
        print(f"Test set: {len(self.test_dataset)} images")

        # Access class information
        print(f"Classes: {self.train_dataset.classes}")  # ['NORMAL', 'PNEUMONIA']
        print(f"Class mapping: {self.train_dataset.class_to_idx}")  # {'NORMAL': 0, 'PNEUMONIA': 1}

    def test_model(self,model_input, test_loader_input):

            model = model_input
            test_loader = test_loader_input

            test_loss = 0.0
            correct = 0
            total = 0

            # Set model to evaluation mode
            model.eval()

            with torch.no_grad():
                for inputs, labels in test_loader:
                    inputs, labels = inputs.to(self.device), labels.to(self.device)

                    # Forward pass
                    outputs = model(inputs)
                    loss = F.cross_entropy(outputs, labels)
                    test_loss += loss.item()

                    # (value, indices)
                    _, preds = torch.max(outputs, 1)
                    correct += (preds == labels).sum().item()
                    total += labels.size(0)

            # Calculate final metrics
            test_loss = test_loss / len(test_loader)
            test_accuracy = correct / total


            return test_loss, test_accuracy


    def eval_model(self, model_input, eval_loader_input):

        model = model_input
        eval_loader = eval_loader_input

        # Initialize variables for tracking performance

        model.eval()

        running_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in eval_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = model(inputs)
                loss = F.cross_entropy(outputs, labels)
                running_loss += loss.item() * inputs.size(0)

                # Get predictions
                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        # Calculate metrics
        eval_loss = running_loss / len(eval_loader)
        eval_accuracy = correct / total



        return eval_loss, eval_accuracy
    
    def evaluator(self, model_input, loader_input):

        model = model_input
        loader = loader_input

        # Initialize variables for tracking performance

        model.eval()
        all_preds = []
        all_labels = []
        all_probs = []

        with torch.no_grad():
            for images, labels in loader:
                images, labels = images.to(self.device), labels.to(self.device)

                outputs = model(images)
                probs = torch.softmax(outputs, dim=1)[:,1]
                preds = torch.argmax(outputs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
                

        return all_labels, all_preds, all_probs


    def compute_metrics(self, labels, preds):
        precisions = precision_score(labels, preds, average=None)
        recalls = recall_score(labels, preds, average=None)
        F1s = f1_score(labels, preds, average=None)
        
        return precisions, recalls, F1s
    
    def plot_roc(self, labels, probs):
        fpr, tpr, _ = roc_curve(labels, probs)
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(6, 6))
        plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
        plt.plot([0, 1], [0, 1], linestyle="--")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title("ROC Curve")
        plt.legend()
        plt.grid(True)
        
        plt.savefig("roc_curve_DenseNet.png")
        
        plt.show()
        
    def create_confusion_matrix_plot(self, all_labels, all_preds, filename="confusion_matrix_DenseNet161.png"):

        cm = confusion_matrix(all_labels, all_preds)
        conf_matrix = ConfusionMatrixDisplay(cm, display_labels=["NORMAL", "PNEUMONIA"])
        fig, ax = plt.subplots(figsize=(6,5))
        conf_matrix.plot(ax=ax, cmap="Blues")
        ax.set_ylabel("True Label")
        ax.set_xlabel("Predicted Label")
        ax.set_title('Confusion Matrix')
        plt.tight_layout()
        plt.savefig(filename, dpi=300)
        plt.show()

    def train_model(self,model_input, train_loader_input, val_loader_input, optimizer_input, epochs=20):

        model = model_input
        train_loader = train_loader_input
        val_loader = val_loader_input
        optimizer = optimizer_input

        # Training history
        train_losses = []
        train_accuracies = []
        val_losses = []
        val_accuracies = []

        for epoch in range(epochs):
            # Training phase
            model.train()
            running_loss = 0.0
            train_correct = 0
            train_total = 0

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                print(inputs.shape)
                print(labels.shape)
                # Zero gradients
                optimizer.zero_grad()

                # Forward pass
                outputs = model(inputs)
                loss = F.cross_entropy(outputs, labels)

                # Backward pass
                loss.backward()
                optimizer.step()

                # Track metrics
                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()

            # Calculate epoch metrics
            epoch_loss = running_loss / len(train_loader)
            epoch_acc = train_correct / train_total
            train_losses.append(epoch_loss)
            train_accuracies.append(epoch_acc)

            # Validation phase
            val_loss, val_acc = self.eval_model(model, val_loader)
            val_losses.append(val_loss)
            val_accuracies.append(val_acc)

            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                print("======================================================================================================")
                print("saving best model......")
                print("======================================================================================================")

                # Save relevant information
                checkpoint = {
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'epoch': epoch,
                    'val_loss': val_loss,
                    'val_acc': val_acc,
                    'train_losses': train_losses,
                    'val_losses': val_losses
                }

                torch.save(checkpoint, 'best_model_full.pt')



            # Print epoch results
            print(f'Epoch {epoch + 1}/{epochs}:')
            print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\n')

        return {
            'train_losses': train_losses,
            'train_accuracies': train_accuracies,
            'val_losses': val_losses,
            'val_accuracies': val_accuracies
        }

    def print_model_parameters(self):
        
        print("="*50)
        print(f'PARAMETERS BY LAYER')
        print("="*50)
        
        trainable_params = 0
        non_trainable_params = 0
    
        # Iterate through all parameters
        for param in self.model.parameters():
            if param.requires_grad:
                trainable_params += param.numel()
            else:
                non_trainable_params += param.numel()
                
        return trainable_params, non_trainable_params
    

    def progressive_training_DenseNet161(self,model_input, train_loader_input, val_loader_input):

        model = model_input
        train_loader = train_loader_input
        val_loader = val_loader_input

        for param in model.parameters():
            param.requires_grad = False

        print("Unfreezing denseblock1...")
        for param in model.features.denseblock1.parameters():
            param.requires_grad = True

        print("Unfreezing transition1...")
        print()
        print()
        for param in model.features.transition1.parameters():
            param.requires_grad = True

        optimizer = optim.SGD([
            {'params': model.features.denseblock1.parameters(), 'lr': 0.02},
            {'params': model.features.transition1.parameters(), 'lr': 0.05}
        ])

        # Train
        print()
        print()
        print("=======================================================================")
        print("Training denseblock1 and transition1...")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=2)

        #Freeze all
        for param in model.parameters():
            param.requires_grad = False

        print("Unfreezing denseblock2...")
        for param in model.features.denseblock2.parameters():
            param.requires_grad = True

        # optimizer
        optimizer = optim.SGD(
            model.features.denseblock2.parameters(), lr = 0.02)

        # Train
        print("=======================================================================")
        print("Training denseblock2...")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=3)

        #Freeze all
        for param in model.parameters():
            param.requires_grad = False

        print("Unfreezing transition2...")
        for param in model.features.transition2.parameters():
            param.requires_grad = True

        print("Unfreezing denseblock3...")
        for param in model.features.denseblock3.parameters():
            param.requires_grad = True

        optimizer = optim.SGD([
            {'params': model.features.transition2.parameters(), 'lr': 0.05},
            {'params': model.features.denseblock3.parameters(), 'lr': 0.02}
        ])

        # Train
        print("=======================================================================")
        print("Training transition2 and denseblock3")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=3)

        #Freeze all
        for param in model.parameters():
            param.requires_grad = False

        print("Unfreezing transition3...")
        for param in model.features.transition3.parameters():
            param.requires_grad = True

        print("Unfreezing denseblock4...")
        for param in model.features.denseblock4.parameters():
            param.requires_grad = True

        # Initial optimizer
        optimizer = optim.SGD([
            {'params': model.features.transition3.parameters(), 'lr': 0.05},
            {'params': model.features.denseblock4.parameters(), 'lr': 0.02}
        ])

        #Train
        print("=======================================================================")
        print("Training transition3 and denseblock4")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=3)

        # Freeze all
        for param in model.parameters():
            param.requires_grad = False

        # Unfreeze norm5
        for param in model.features.norm5.parameters():
            param.requires_grad = True

        # Unfreeze classifier
        for param in model.classifier.parameters():
            param.requires_grad = True

        # Initial optimizer
        optimizer = optim.SGD([
            {'params': model.features.norm5.parameters(), 'lr': 0.05},
            {'params': model.classifier.parameters(), 'lr': 0.02}
        ])


        #Train
        print("Training norm5 and classifier")
        print("=======================================================================")
        self.train_model(model, train_loader, val_loader, optimizer, epochs=4)
        print("=======================================================================")


    def load_and_use_model_DenseNet(self,path = 'best_model_full_DenseNet.pt', device=torch.device("cpu")):

        #Set up device
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        #Load the checkpoint
        checkpoint = torch.load(path)

        #Initialize the model architecture
        model = densenet161(pretrained=True)
        model.classifier = nn.Linear(2208, 2)
        model = model.to(device)

        #Load the saved state dict
        model.load_state_dict(checkpoint['model_state_dict'])

        #Set model to evaluation mode
        model.eval()

        #Transform for new images
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
        ])

        return model, transform
    
    def print_checkpoint_values(self, checkpoint_path='best_model_full_DenseNet.pt'):

        # Load checkpoint
        checkpoint = torch.load(checkpoint_path)
        
        print("="*50)
        print("CHECKPOINT VALUES")
        print("="*50)
        
        # Print training info
        print(f"Best epoch: {checkpoint['epoch']}")
        print(f"Validation Loss: {checkpoint['val_loss']:.4f}")
        print(f"Validation Accuracy: {checkpoint['val_acc']:.4f}")

    def getActivation(self, name):
        def hook(module, input, output):
            self.activation[name] = output.detach()
        return hook


    def predict_image(self, model, test_loader_input):
        test_loader = test_loader_input
        model.features.denseblock4.denselayer24.conv2.register_forward_hook(self.getActivation('final_conv'))
        # Load and preprocess the image
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        for image, label in test_loader:
            image_tensor = image.to(device)

            # Make prediction
            with torch.no_grad():
                outputs = model(image_tensor)
                _, predicted = torch.max(outputs, 1)
                
                
                # Fetch the learned weights at the final feed-forward layer
                weight_fc = model.classifier.weight.detach().numpy()
                print(weight_fc.shape)


                # Fetch feature maps at the final convolutional layer
                conv_features = self.activation['final_conv']
                print(f"Feature map shape: {conv_features.shape}")
                print(f"Model predicted value {predicted.item()}")
                print(f"Actual value {label.item()}")
                print("-" * 50)
                
    def collect_model_info(self, model_input, test_loader_input):
        
        model = model_input
        test_loader = test_loader_input

        # 1. Model Architecture
        print("Model Architecture:")
        print(model)
        print("================================================================================")

        # 2. Check Frozen Layers
        print("CHECKING FROZEN AND UNFROZEN LAYERS")
        print("================================================================================")
        for name, param in model.named_parameters():
            if not param.requires_grad:
                print(f"Layer {name} is frozen")
            else:
                print(f"Layer {name} is unfrozen")
        print("================================================================================")

        # 3. Dataset Class Information
        print("\nDataset sizes:")
        print(f"Training set: {len(self.train_dataset)} images")
        print(f"Validation set: {len(self.val_dataset)} images")
        print(f"Test set: {len(self.test_dataset)} images")
        print(f"Classes: {self.train_dataset.classes}")
        print(f"Class mapping: {self.train_dataset.class_to_idx}")
        print("================================================================================")

        # 4. Test Loss and Accuracy
        test_loss, test_accuracy = self.test_model(model, test_loader)
        print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")
        print("================================================================================")



def main():

    model = DenseNet161()
    model.init_dataloader()
    test_loader = model.test_loader
    model_fined_tuned, transform = model.load_and_use_model_DenseNet()
    model.collect_model_info(model_fined_tuned, test_loader)


# def main():
#     model = DenseNet161()
    
#     model.print_checkpoint_values()

# def main():

#     model = DenseNet161()
#     model.init_dataloader()
#     model_fined_tuned, transform = model.load_and_use_model_DenseNet()
#     print(model_fined_tuned)
    
    
# def main():

#     model = DenseNet161()
#     model.init_dataloader()
#     test_loader = model.test_loader
#     model_fined_tuned, transform = model.load_and_use_model_DenseNet()
#     model.predict_image(model_fined_tuned, test_loader)

# def main():

#     model = DenseNet161()
#     model.init_dataloader()
#     test_loader = model.test_loader
#     model.print_model()
#     model.check_frozen_layer()
#     model.print_dataset_class_info()
#     model.progressive_training_DenseNet161(model.model, model.train_loader, model.val_loader)
#     model.test_model(model.model, test_loader)
    
    
if __name__ == '__main__':
    main()